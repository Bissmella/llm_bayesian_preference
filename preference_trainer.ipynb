{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOzqoRoI5sYn"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVh4wHzZ2PO-"
      },
      "source": [
        "This notebook is a simplified notebook version of the implementation bayesian alignment of LLM\n",
        "\n",
        "\n",
        "# all the data and files are assumed to be inside /content/ ..\n",
        "\n",
        "# TODO: probably generate a loss graph for different loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T14:21:25.133934Z",
          "iopub.status.busy": "2025-03-20T14:21:25.133724Z",
          "iopub.status.idle": "2025-03-20T14:21:25.137565Z",
          "shell.execute_reply": "2025-03-20T14:21:25.136806Z",
          "shell.execute_reply.started": "2025-03-20T14:21:25.133903Z"
        },
        "id": "Ve2a7Zj4FoXm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "PARENT = \"/kaggle/input/preference-learning/\" #directory including the pre-processed data and config files\n",
        "OUTPUT = \"/kaggle/working/\" #output directory where model, losses, and configs will be saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T14:21:25.163761Z",
          "iopub.status.busy": "2025-03-20T14:21:25.163387Z",
          "iopub.status.idle": "2025-03-20T14:21:33.340455Z",
          "shell.execute_reply": "2025-03-20T14:21:33.339673Z",
          "shell.execute_reply.started": "2025-03-20T14:21:25.163732Z"
        },
        "id": "2tN0no9j50np",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import (AutoTokenizer, AutoModel, AutoConfig)\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T14:21:33.343077Z",
          "iopub.status.busy": "2025-03-20T14:21:33.342719Z",
          "iopub.status.idle": "2025-03-20T14:21:33.392392Z",
          "shell.execute_reply": "2025-03-20T14:21:33.391458Z",
          "shell.execute_reply.started": "2025-03-20T14:21:33.343056Z"
        },
        "id": "RcY08JWr6AnI",
        "outputId": "fd5f0bc0-587f-4eb2-a56e-6f12c5273cca",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2025-03-20T14:21:33.394323Z",
          "iopub.status.busy": "2025-03-20T14:21:33.393974Z",
          "iopub.status.idle": "2025-03-20T14:21:45.453055Z",
          "shell.execute_reply": "2025-03-20T14:21:45.452367Z",
          "shell.execute_reply.started": "2025-03-20T14:21:33.394276Z"
        },
        "id": "aFSFeKmyL0R7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title bert model commons\n",
        "import math\n",
        "import inspect\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import (List, Optional, Tuple, Union, Callable)\n",
        "from transformers import (PreTrainedTokenizer, PreTrainedModel, PretrainedConfig)\n",
        "from transformers.models.bert import (BertConfig, BertModel, BertLayer, )\n",
        "from transformers.models.bert.modeling_bert import (BertAttention, BertSelfAttention, )\n",
        "from transformers.modeling_outputs import (BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "                                           BaseModelOutputWithPastAndCrossAttentions)\n",
        "\n",
        "\n",
        "class BaseModel(PreTrainedModel):\n",
        "    def __init__(self, config: PretrainedConfig):\n",
        "        super().__init__(config)\n",
        "        self.toker = None\n",
        "\n",
        "    def tie_tokenizer(self, toker: PreTrainedTokenizer):\n",
        "        self.toker = toker\n",
        "        if len(self.toker) > self.toker.vocab_size:\n",
        "            self.resize_token_embeddings(len(self.toker))\n",
        "\n",
        "\n",
        "class PrefixBertSelfAttention(BertSelfAttention):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            hidden_states: torch.Tensor,\n",
        "            attention_mask: Optional[torch.FloatTensor] = None,\n",
        "            head_mask: Optional[torch.FloatTensor] = None,\n",
        "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "            output_attentions: Optional[bool] = False,\n",
        "            prefix: Optional[torch.Tensor] = None,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        the main bert self-attention module acts as both self and cross-attention.\n",
        "\n",
        "        query = query(hidden_states)\n",
        "        if cross-attention and past_key_values:\n",
        "          key, value = past_key_value[0], [1]\n",
        "        elif cross-attention:\n",
        "          key, value = key/value(encoder_hidden_states)\n",
        "        elif past_key_values != None:\n",
        "          key, value = past_key_value[0]/[1] + key/value(hidden_states)  ##concat\n",
        "        else:\n",
        "          key, value = key/value(hidden_states)\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        batch_size, seq_length = hidden_states.shape[:2]\n",
        "        real_seq_length = seq_length\n",
        "\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        use_cache = past_key_value is not None\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        if prefix is not None:\n",
        "            key_layer = torch.cat([prefix[\"prev_key\"], key_layer], dim=2)\n",
        "            value_layer = torch.cat([prefix[\"prev_value\"], value_layer], dim=2)  # TODO: Chen\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n",
        "            if use_cache:\n",
        "                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n",
        "                    -1, 1\n",
        "                )\n",
        "            else:\n",
        "                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        #########################################\n",
        "        if prefix is not None:\n",
        "            assert key_layer.shape[2] > attention_mask.shape[3]\n",
        "            assert prefix[\"prev_key_padding_mask\"].shape[1] == key_layer.shape[2] - real_seq_length\n",
        "            assert attention_mask.shape[3] == real_seq_length\n",
        "            attention_mask = torch.cat([prefix[\"prev_key_padding_mask\"].float().unsqueeze(1).unsqueeze(2).\n",
        "                                       expand(-1, -1, attention_mask.shape[2], -1) * -10000.0, attention_mask], dim=3)\n",
        "        #########################################\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class PrefixBertAttention(BertAttention):\n",
        "    \"\"\"\n",
        "    just applying the attention above and the last linear and residual in a normal attention layer.\n",
        "    outputing attention probs as second argument\n",
        "    \"\"\"\n",
        "    def __init__(self, config, position_embedding_type=None):\n",
        "        super().__init__(config, position_embedding_type)\n",
        "        self.self = PrefixBertSelfAttention(config)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            hidden_states: torch.Tensor,\n",
        "            attention_mask: Optional[torch.FloatTensor] = None,\n",
        "            head_mask: Optional[torch.FloatTensor] = None,\n",
        "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "            output_attentions: Optional[bool] = False,\n",
        "            prefix: Optional[torch.Tensor] = None,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "            prefix,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class PrefixBertLayer(BertLayer):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.attention = PrefixBertAttention(config)\n",
        "        if self.add_cross_attention:\n",
        "            if not self.is_decoder:\n",
        "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
        "            self.crossattention = PrefixBertAttention(config, position_embedding_type=\"absolute\")\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            hidden_states: torch.Tensor,\n",
        "            attention_mask: Optional[torch.FloatTensor] = None,\n",
        "            head_mask: Optional[torch.FloatTensor] = None,\n",
        "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "            output_attentions: Optional[bool] = False,\n",
        "            encoder_prefix: Optional[torch.Tensor] = None,\n",
        "            decoder_prefix: Optional[torch.Tensor] = None,\n",
        "            cross_attn_prefix: Optional[torch.Tensor] = None,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "            prefix=decoder_prefix if self.is_decoder else encoder_prefix,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        # if decoder, the last output is tuple of self-attn cache\n",
        "        if self.is_decoder:\n",
        "            outputs = self_attention_outputs[1:-1]\n",
        "            present_key_value = self_attention_outputs[-1]\n",
        "        else:\n",
        "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        cross_attn_present_key_value = None\n",
        "        if self.is_decoder and encoder_hidden_states is not None:\n",
        "            if not hasattr(self, \"crossattention\"):\n",
        "                raise ValueError(\n",
        "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n",
        "                    \" by setting `config.add_cross_attention=True`\"\n",
        "                )\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            cross_attention_outputs = self.crossattention(\n",
        "                attention_output,\n",
        "                attention_mask=attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                encoder_hidden_states=encoder_hidden_states,\n",
        "                encoder_attention_mask=encoder_attention_mask,\n",
        "                past_key_value=cross_attn_past_key_value,\n",
        "                output_attentions=output_attentions,\n",
        "                prefix=cross_attn_prefix,\n",
        "            )\n",
        "            attention_output = cross_attention_outputs[0]\n",
        "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
        "\n",
        "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
        "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        layer_output = apply_chunking_to_forward(\n",
        "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
        "        )\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        # if decoder, return the attn key/values as the last output\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "class PrefixBertEncoder(nn.Module):\n",
        "    def __init__(self, config: BertConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([PrefixBertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            hidden_states: torch.Tensor,\n",
        "            past_prompt: torch.Tensor,\n",
        "            attention_mask: Optional[torch.FloatTensor] = None,\n",
        "            head_mask: Optional[torch.FloatTensor] = None,\n",
        "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "            past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "            use_cache: Optional[bool] = None,\n",
        "            output_attentions: Optional[bool] = False,\n",
        "            output_hidden_states: Optional[bool] = False,\n",
        "            return_dict: Optional[bool] = True,\n",
        "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        prefix_l = len(self.layer) - len(past_prompt)\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            encoder_prefix = None\n",
        "            decoder_prefix = None\n",
        "            cross_attn_prefix = None\n",
        "            if i >= prefix_l and past_prompt:\n",
        "                encoder_prefix = past_prompt[i - prefix_l].get('encoder_prompt', None)\n",
        "                decoder_prefix = past_prompt[i - prefix_l].get('decoder_prompt', None)\n",
        "                cross_attn_prefix = past_prompt[i - prefix_l].get('cross_attention_prompt', None)\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                    encoder_prefix=encoder_prefix,\n",
        "                    decoder_prefix=decoder_prefix,\n",
        "                    cross_attn_prefix=cross_attn_prefix,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class PrefixBertModel(BertModel):\n",
        "    def __init__(self, config: BertConfig, add_pooling_layer=True):\n",
        "        super().__init__(config, add_pooling_layer)\n",
        "        self.encoder = PrefixBertEncoder(config)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids: Optional[torch.Tensor] = None,\n",
        "            past_prompt: Optional[torch.Tensor] = None,\n",
        "            attention_mask: Optional[torch.Tensor] = None,\n",
        "            token_type_ids: Optional[torch.Tensor] = None,\n",
        "            position_ids: Optional[torch.Tensor] = None,\n",
        "            head_mask: Optional[torch.Tensor] = None,\n",
        "            inputs_embeds: Optional[torch.Tensor] = None,\n",
        "            encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "            encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "            past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "            use_cache: Optional[bool] = None,\n",
        "            output_attentions: Optional[bool] = None,\n",
        "            output_hidden_states: Optional[bool] = None,\n",
        "            return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
        "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
        "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
        "        use_cache (`bool`, *optional*):\n",
        "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
        "            `past_key_values`).\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
        "\n",
        "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length,\n",
        "        )\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            past_prompt=past_prompt,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            past_key_values=encoder_outputs.past_key_values,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "            cross_attentions=encoder_outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "def apply_chunking_to_forward(\n",
        "        forward_fn: Callable[..., torch.Tensor], chunk_size: int, chunk_dim: int, *input_tensors\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This function chunks the `input_tensors` into smaller input tensor parts of size `chunk_size` over the dimension\n",
        "    `chunk_dim`. It then applies a layer `forward_fn` to each chunk independently to save memory.\n",
        "    If the `forward_fn` is independent across the `chunk_dim` this function will yield the same result as directly\n",
        "    applying `forward_fn` to `input_tensors`.\n",
        "    Args:\n",
        "        forward_fn (`Callable[..., torch.Tensor]`):\n",
        "            The forward function of the model.\n",
        "        chunk_size (`int`):\n",
        "            The chunk size of a chunked tensor: `num_chunks = len(input_tensors[0]) / chunk_size`.\n",
        "        chunk_dim (`int`):\n",
        "            The dimension over which the `input_tensors` should be chunked.\n",
        "        input_tensors (`Tuple[torch.Tensor]`):\n",
        "            The input tensors of `forward_fn` which will be chunked\n",
        "    Returns:\n",
        "        `torch.Tensor`: A tensor with the same shape as the `forward_fn` would have given if applied`.\n",
        "    Examples:\n",
        "    ```python\n",
        "    # rename the usual forward() fn to forward_chunk()\n",
        "    def forward_chunk(self, hidden_states):\n",
        "        hidden_states = self.decoder(hidden_states)\n",
        "        return hidden_states\n",
        "    # implement a chunked forward function\n",
        "    def forward(self, hidden_states):\n",
        "        return apply_chunking_to_forward(self.forward_chunk, self.chunk_size_lm_head, self.seq_len_dim, hidden_states)\n",
        "    ```\"\"\"\n",
        "\n",
        "    assert len(input_tensors) > 0, f\"{input_tensors} has to be a tuple/list of tensors\"\n",
        "\n",
        "    # inspect.signature exist since python 3.5 and is a python method -> no problem with backward compatibility\n",
        "    num_args_in_forward_chunk_fn = len(inspect.signature(forward_fn).parameters)\n",
        "    if num_args_in_forward_chunk_fn != len(input_tensors):\n",
        "        raise ValueError(\n",
        "            f\"forward_chunk_fn expects {num_args_in_forward_chunk_fn} arguments, but only {len(input_tensors)} input \"\n",
        "            \"tensors are given\"\n",
        "        )\n",
        "\n",
        "    if chunk_size > 0:\n",
        "        tensor_shape = input_tensors[0].shape[chunk_dim]\n",
        "        for input_tensor in input_tensors:\n",
        "            if input_tensor.shape[chunk_dim] != tensor_shape:\n",
        "                raise ValueError(\n",
        "                    f\"All input tenors have to be of the same shape: {tensor_shape}, \"\n",
        "                    f\"found shape {input_tensor.shape[chunk_dim]}\"\n",
        "                )\n",
        "\n",
        "        if input_tensors[0].shape[chunk_dim] % chunk_size != 0:\n",
        "            raise ValueError(\n",
        "                f\"The dimension to be chunked {input_tensors[0].shape[chunk_dim]} has to be a multiple of the chunk \"\n",
        "                f\"size {chunk_size}\"\n",
        "            )\n",
        "\n",
        "        num_chunks = input_tensors[0].shape[chunk_dim] // chunk_size\n",
        "\n",
        "        # chunk input tensor into tuples\n",
        "        input_tensors_chunks = tuple(input_tensor.chunk(num_chunks, dim=chunk_dim) for input_tensor in input_tensors)\n",
        "        # apply forward fn to every tuple\n",
        "        output_chunks = tuple(forward_fn(*input_tensors_chunk) for input_tensors_chunk in zip(*input_tensors_chunks))\n",
        "        # concatenate output at same dimension\n",
        "        return torch.cat(output_chunks, dim=chunk_dim)\n",
        "\n",
        "    return forward_fn(*input_tensors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T14:21:45.454366Z",
          "iopub.status.busy": "2025-03-20T14:21:45.453881Z",
          "iopub.status.idle": "2025-03-20T14:21:45.47912Z",
          "shell.execute_reply": "2025-03-20T14:21:45.478078Z",
          "shell.execute_reply.started": "2025-03-20T14:21:45.454341Z"
        },
        "id": "s6oa39dML6R3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title model itself\n",
        "\n",
        "\n",
        "from transformers import BertForSequenceClassification #BertConfig, BertModel,\n",
        "def ce_loss(preds, prior):\n",
        "    return -(preds * prior).sum(1), torch.Tensor([0])\n",
        "\n",
        "\n",
        "def comb_loss(preds, prior):\n",
        "    return -(preds.softmax(1) * prior / prior.sum(1).unsqueeze(1)).sum(1).log(), torch.Tensor([0])\n",
        "\n",
        "def comp_r (preds, priors, eps=1e-9):\n",
        "    logprior = priors.clamp(min=eps).log()\n",
        "    r = (preds.log_softmax(0) + logprior).log_softmax(1)\n",
        "    return r\n",
        "\n",
        "def comp_kl_r(preds, mix, prior=False):\n",
        "    if prior:\n",
        "        preds = preds.clamp(min=eps).log()\n",
        "    t1 = -(mix * preds.exp()).sum(1)\n",
        "    t2 = -(preds * preds.exp()).sum(1)\n",
        "    return t1.mean() - t2.mean()\n",
        "\n",
        "def qr_loss(preds, prior, eps=1e-9):\n",
        "    logprior = prior.clamp(min=eps).log()\n",
        "    r = (preds.log_softmax(0) + logprior).log_softmax(1)\n",
        "    return -(r * preds.exp()).sum(1), -(preds * preds.exp()).sum(1)\n",
        "\n",
        "\n",
        "def rq_loss(preds, prior, eps=1e-9):\n",
        "    logprior = prior.clamp(min=eps).log()\n",
        "    r = (preds.log_softmax(0) + logprior).log_softmax(1)\n",
        "    return -(preds * r.exp()).sum(1), -(r * r.exp()).sum(1)\n",
        "\n",
        "\n",
        "class Model(BaseModel, BertForSequenceClassification):\n",
        "    def __init__(self, config: BertConfig, args):\n",
        "        super().__init__(config)\n",
        "        self.n_embd = self.config.hidden_size\n",
        "        self.mid_dim = args.d_prefix\n",
        "        self.match_n_layer = self.config.num_hidden_layers // 2\n",
        "        self.match_n_head = self.config.num_attention_heads\n",
        "        self.match_n_embd = self.config.hidden_size // self.config.num_attention_heads\n",
        "        self.hidden_size = self.config.hidden_size\n",
        "        self.class_num = args.class_num\n",
        "        self.loss_fn = args.loss_fn\n",
        "\n",
        "        # The Multi prefix modules!\n",
        "        # The task-prefix modules from all specific tasks\n",
        "        self.prefix_names = [\"posterior\"]\n",
        "        self.preseqlen = args.preseqlen\n",
        "        self.prefix_tokens = torch.arange(self.preseqlen).long()\n",
        "        self.input_tokens = torch.arange(self.preseqlen).long()\n",
        "        self.multi_prefix = nn.ModuleDict(\n",
        "            {\n",
        "                name: nn.ModuleDict(\n",
        "                    {\n",
        "                        \"wte_enc\": nn.Embedding(self.preseqlen, self.n_embd),\n",
        "                        \"control_trans_enc\": nn.Sequential(\n",
        "                            nn.Linear(self.n_embd, self.mid_dim),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(self.mid_dim, self.match_n_layer * 2 * self.match_n_head * self.match_n_embd),\n",
        "                        ),\n",
        "                        \"mlp\": nn.Sequential(\n",
        "                            nn.Linear(config.hidden_size, self.mid_dim),\n",
        "                            nn.Tanh(),  # nn.LeakyReLU(),\n",
        "                            nn.Linear(self.mid_dim, self.class_num),\n",
        "                        ),\n",
        "                    }\n",
        "                )\n",
        "                for name in self.prefix_names\n",
        "            }\n",
        "        )\n",
        "        self.prefix_dropout = nn.Dropout(args.prefix_dropout)\n",
        "        self.bert = PrefixBertModel(config)\n",
        "        self.classifier = None\n",
        "\n",
        "    def get_distribution(self, labels, bsz=None, smooth=0.1):\n",
        "        \"\"\" Convert labels to label distribution.\n",
        "\n",
        "        :param labels: (bsz, )\n",
        "        :param bsz: batch size\n",
        "        :param smooth: smoothing factor\n",
        "        :return labels_dist: (bsz, class_num)\n",
        "\n",
        "        \"\"\"\n",
        "        class_num = self.class_num\n",
        "        labels_dist = torch.zeros(bsz, class_num + 1).to(labels.device)\n",
        "        one_tensor = torch.ones_like(labels_dist)\n",
        "        label_mask = labels == -100\n",
        "        labels = labels.masked_fill(label_mask, class_num)\n",
        "        labels_dist = labels_dist.scatter_add(1, labels, one_tensor)\n",
        "        labels_dist = labels_dist[:, :-1] + smooth\n",
        "        assert labels_dist.shape == (bsz, class_num)\n",
        "        return labels_dist / labels_dist.sum(-1, keepdim=True)\n",
        "\n",
        "    def get_prompt(self, name, bsz=None, device=\"cuda\"):\n",
        "        old_bsz = bsz\n",
        "\n",
        "        # Encoder prefix\n",
        "        input_tokens_enc = (\n",
        "            self.input_tokens.unsqueeze(0).expand(old_bsz, -1).to(device)\n",
        "        )\n",
        "        temp_control_enc = self.multi_prefix[name][\"wte_enc\"](input_tokens_enc)\n",
        "        temp_control_enc = self.prefix_dropout(temp_control_enc)\n",
        "        past_key_values_enc = self.multi_prefix[name][\"control_trans_enc\"](\n",
        "            temp_control_enc\n",
        "        )  # bsz, seqlen, layer*emb\n",
        "        past_key_values_enc = self.prefix_dropout(past_key_values_enc)\n",
        "        bsz_enc, seqlen, _ = past_key_values_enc.shape\n",
        "        past_key_values_enc = past_key_values_enc.view(\n",
        "            bsz_enc,\n",
        "            seqlen,\n",
        "            self.match_n_layer * 2,\n",
        "            self.match_n_head,\n",
        "            self.match_n_embd,\n",
        "        )\n",
        "        past_key_values_enc = past_key_values_enc.permute([2, 0, 3, 1, 4]).split(2)\n",
        "\n",
        "        result = []\n",
        "        for i, key_val_enc in enumerate(past_key_values_enc):\n",
        "            temp = dict()\n",
        "            temp[\"encoder_prompt\"] = {\n",
        "                \"prev_key\": key_val_enc[0].contiguous(),\n",
        "                \"prev_value\": key_val_enc[1].contiguous(),\n",
        "                \"prev_key_padding_mask\": torch.zeros(bsz_enc, seqlen).to(device).bool(),\n",
        "            }\n",
        "            result.append(temp)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_ids: Optional[torch.Tensor] = None,\n",
        "            attention_mask: Optional[torch.Tensor] = None,\n",
        "            token_type_ids: Optional[torch.Tensor] = None,\n",
        "            position_ids: Optional[torch.Tensor] = None,\n",
        "            head_mask: Optional[torch.Tensor] = None,\n",
        "            inputs_embeds: Optional[torch.Tensor] = None,\n",
        "            labels: Optional[torch.Tensor] = None,\n",
        "            scores: Optional[torch.Tensor] = None,\n",
        "            validation=False,\n",
        "            **kwargs,\n",
        "    ):\n",
        "        assert self.toker != None\n",
        "\n",
        "        bsz = input_ids.size(0)\n",
        "        distribution = {}\n",
        "        past_prompt = self.get_prompt(\"posterior\", bsz, device=input_ids.device)\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            past_prompt,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_hidden_states=True,\n",
        "        )[0]\n",
        "        masks = input_ids == 0 if token_type_ids == None else token_type_ids == 0\n",
        "        pooler = outputs.masked_fill(masks.unsqueeze(-1), 0)\n",
        "        pooler = pooler.sum(dim=1) / (1 - masks.long()).sum(dim=1).unsqueeze(-1)\n",
        "        distribution[\"posterior\"] = self.multi_prefix[\"posterior\"][\"mlp\"](pooler).log_softmax(-1)\n",
        "\n",
        "        loss = None\n",
        "        if labels != None:\n",
        "            distribution[\"prior\"] = self.get_distribution(labels, bsz)\n",
        "            if self.loss_fn == \"rq\":\n",
        "                loss = self.comp_rq_loss(distribution)\n",
        "            elif self.loss_fn == \"qr\":\n",
        "                loss = self.comp_qr_loss(distribution)\n",
        "            else: #Janson shanon divergence\n",
        "                loss = self.comp_js_loss(distribution)\n",
        "        elif scores != None:\n",
        "            distribution[\"prior\"] = scores\n",
        "            if self.loss_fn == \"rq\":\n",
        "                loss = self.comp_rq_loss(distribution)\n",
        "            elif self.loss_fn == \"qr\":\n",
        "                loss = self.comp_qr_loss(distribution)\n",
        "            else: #Janson shanon divergence\n",
        "                loss = self.comp_js_loss(distribution)\n",
        "        res = {\"dist\": distribution[\"posterior\"].exp(), \"loss\": loss}\n",
        "        if not self.training and not validation:\n",
        "            return res\n",
        "        else:\n",
        "            assert not validation\n",
        "        return res\n",
        "\n",
        "    ### compute loss\n",
        "    def comp_rq_loss(self, distribution):\n",
        "        posterior, prior = distribution[\"posterior\"], distribution[\"prior\"]\n",
        "        l1, l2 = rq_loss(posterior, prior)\n",
        "        l = l1.mean() - l2.mean()\n",
        "        return l\n",
        "\n",
        "    def comp_qr_loss(self, distribution):\n",
        "        posterior, prior = distribution[\"posterior\"], distribution[\"prior\"]\n",
        "        l1, l2 = qr_loss(posterior, prior)\n",
        "        l = l1.mean() - l2.mean()\n",
        "        return l\n",
        "\n",
        "    def comp_js_loss(self, distribution): # Jensen-Shannon divergence\n",
        "        posterior, prior = distribution[\"posterior\"], distribution[\"prior\"]\n",
        "        ar = comp_r(posterior, prior)\n",
        "        mixture = 0.5 * (posterior + ar)\n",
        "        l_posterior = comp_kl_r(posterior, mixture)\n",
        "        l_prior = comp_kl_r(ar, mixture)\n",
        "        return 0.5 * (l_posterior + l_prior)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(\n",
        "            self,\n",
        "            input_ids: Optional[torch.Tensor] = None,\n",
        "            attention_mask: Optional[torch.Tensor] = None,\n",
        "            token_type_ids: Optional[torch.Tensor] = None,\n",
        "            position_ids: Optional[torch.Tensor] = None,\n",
        "            head_mask: Optional[torch.Tensor] = None,\n",
        "            inputs_embeds: Optional[torch.Tensor] = None,\n",
        "            **kwargs,\n",
        "    ):\n",
        "        assert not self.training\n",
        "        assert self.toker != None\n",
        "\n",
        "        bsz = input_ids.size(0)\n",
        "        past_prompt = self.get_prompt(\"posterior\", bsz, device=input_ids.device)\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            past_prompt,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_hidden_states=True,\n",
        "        )[0]\n",
        "\n",
        "        masks = input_ids == 0 if token_type_ids == None else token_type_ids == 0\n",
        "        pooler = outputs.masked_fill(masks.unsqueeze(-1), 0)\n",
        "        pooler = pooler.sum(dim=1) / (1 - masks.long()).sum(dim=1).unsqueeze(-1)\n",
        "        distribution = self.multi_prefix[\"posterior\"][\"mlp\"](pooler).softmax(-1)\n",
        "\n",
        "        return distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T14:21:45.480478Z",
          "iopub.status.busy": "2025-03-20T14:21:45.48012Z",
          "iopub.status.idle": "2025-03-20T14:21:45.633523Z",
          "shell.execute_reply": "2025-03-20T14:21:45.632488Z",
          "shell.execute_reply.started": "2025-03-20T14:21:45.480446Z"
        },
        "id": "ipU7MNFoPAMr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title data utils\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "from functools import partial\n",
        "from torch.utils.data import DataLoader, Sampler\n",
        "\n",
        "\n",
        "def _norm(s):\n",
        "    return ' '.join(s.strip().split())\n",
        "\n",
        "\n",
        "class BucketSampler(Sampler):\n",
        "    \"\"\"\n",
        "    this sampler will sort data by sequence length\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lens, bucket_size, batch_size, droplast=False, shuffle=True):\n",
        "        self._lens = lens\n",
        "        self._batch_size = batch_size\n",
        "        self._bucket_size = bucket_size\n",
        "        self._droplast = droplast\n",
        "        self._shuf = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        ids = list(range(len(self._lens)))\n",
        "        if self._shuf:\n",
        "            random.shuffle(ids)\n",
        "        buckets = [sorted(ids[i:i + self._bucket_size], key=lambda i: self._lens[i], reverse=True) for i in\n",
        "                   range(0, len(ids), self._bucket_size)]\n",
        "        batches = [bucket[i:i + self._batch_size] for bucket in buckets for i in\n",
        "                   range(0, len(bucket), self._batch_size)]\n",
        "        if self._droplast:\n",
        "            batches = [batch for batch in batches if len(batch) == self._batch_size]\n",
        "        if self._shuf:\n",
        "            random.shuffle(batches)\n",
        "        return iter(batches)\n",
        "\n",
        "    def __len__(self):\n",
        "        bucket_sizes = ([self._bucket_size]\n",
        "                        * (len(self._lens) // self._bucket_size)\n",
        "                        + [len(self._lens) % self._bucket_size])\n",
        "        if self._droplast:\n",
        "            return sum(s // self._batch_size for s in bucket_sizes)\n",
        "        else:\n",
        "            return sum(math.ceil(s / self._batch_size) for s in bucket_sizes)\n",
        "\n",
        "\n",
        "class BucketingDataLoader(object):\n",
        "    def __init__(self, toker, feature_dataset, batch_size, bucket=100, shuffle=True, **kwargs):\n",
        "        assert \"inputter_name\" in kwargs, \"inputter name should be provided\"\n",
        "        inputter_name = kwargs[\"inputter_name\"]\n",
        "        assert inputter_name in [\"off\", \"mic\", \"esc\", \"single\"], \"undefined inputter name\"\n",
        "        if inputter_name == \"off\":\n",
        "            assert \"fine_task\" in kwargs, \"offensive fine task should be provided\"\n",
        "            fine_task = kwargs[\"fine_task\"]\n",
        "            with open(f'dataset/off/{fine_task}_train.pkl', 'rb') as f:\n",
        "                self.data = pickle.load(f)\n",
        "        elif inputter_name == \"mic\":\n",
        "            with open(f'dataset/mic/mic_data_train.pkl', 'rb') as f:\n",
        "                self.data = pickle.load(f)\n",
        "        else:\n",
        "            assert \"fold_num\" in kwargs, \"fold number should be provided when inputter name is esc\"\n",
        "            fold_num = kwargs[\"fold_num\"]\n",
        "            valid_ids = json.load(open(f'{PARENT}dev_index.json', 'r'))[fold_num]\n",
        "            with open(f'{PARENT}{inputter_name}_data.pkl', 'rb') as f:\n",
        "                self.data = [line for line in pickle.load(f) if line.dialog_id not in valid_ids]\n",
        "\n",
        "        self.toker = toker\n",
        "        self.feature_dataset = feature_dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.bucket_size = bucket * batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        trunc_chunk = []\n",
        "        lens = []\n",
        "        for feat in self.data:\n",
        "            trunc_chunk.append(feat)\n",
        "            lens.append(feat.input_length)\n",
        "\n",
        "        dataset = self.feature_dataset(trunc_chunk)\n",
        "        sampler = BucketSampler(lens, self.bucket_size, self.batch_size, droplast=True, shuffle=self.shuffle)\n",
        "        loader = DataLoader(dataset, batch_sampler=sampler, num_workers=0,  # can be multi-worker\n",
        "                            collate_fn=partial(self.feature_dataset.collate, toker=self.toker))\n",
        "        yield from loader\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "class DistributedBucketingDataLoader(BucketingDataLoader):\n",
        "    \"\"\" distributed version \"\"\"\n",
        "\n",
        "    def __init__(self, rank, num_replica, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.rank = rank\n",
        "        self.num_replica = num_replica\n",
        "        self.data = self.data[self.rank::self.num_replica]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T14:21:45.634808Z",
          "iopub.status.busy": "2025-03-20T14:21:45.634566Z",
          "iopub.status.idle": "2025-03-20T14:21:45.662237Z",
          "shell.execute_reply": "2025-03-20T14:21:45.661372Z",
          "shell.execute_reply.started": "2025-03-20T14:21:45.634786Z"
        },
        "id": "w_3d1qXAQNIJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title data esc (smallest)\n",
        "\n",
        "import json\n",
        "import tqdm\n",
        "import random\n",
        "import torch\n",
        "import pickle\n",
        "from typing import List\n",
        "from math import ceil\n",
        "from functools import partial\n",
        "from numpy.random import triangular\n",
        "\n",
        "from torch.utils.data import DataLoader, Sampler, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "class Inputter(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        # prepare\n",
        "        self.convert_data_to_inputs = convert_data_to_inputs\n",
        "        self.convert_inputs_to_features = convert_inputs_to_features\n",
        "\n",
        "        # train\n",
        "        self.train_dataset = FeatureDataset\n",
        "        self.train_dataloader = BucketingDataLoader\n",
        "        self.train_distributed_dataloader = DistributedBucketingDataLoader\n",
        "\n",
        "        # valid\n",
        "        self.valid_dataloader = DynamicBatchingLoader\n",
        "\n",
        "        # infer\n",
        "        self.prepare_infer_batch = prepare_infer_batch\n",
        "        self.infer_dataloader = get_infer_batch\n",
        "\n",
        "\n",
        "# basic utils\n",
        "class InputFeatures(object):\n",
        "    def __init__(self, dialog_id, input_ids, token_type_ids, label):\n",
        "        self.dialog_id = dialog_id\n",
        "        self.input_ids = input_ids\n",
        "        self.input_length = len(input_ids)\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "def featurize(bos, eos, inputs, max_input_length=512):\n",
        "    dialog_id, context, response = inputs[\"dialog_id\"], inputs[\"context\"], inputs[\"response\"]\n",
        "    label = inputs[\"label\"]\n",
        "    response = response + [eos]\n",
        "    response_ids = response[:max_input_length]\n",
        "    context_ids = [bos] + context + [bos]\n",
        "    input_ids = context_ids + response_ids\n",
        "    input_ids = input_ids[-max_input_length:]\n",
        "    token_type_ids = [0] * len(input_ids)\n",
        "    token_type_ids[-len(response_ids):] = [1] * len(response_ids)\n",
        "\n",
        "    assert len(input_ids) == len(token_type_ids)\n",
        "\n",
        "    return InputFeatures(dialog_id, input_ids, token_type_ids, label)\n",
        "\n",
        "\n",
        "# for training\n",
        "def convert_data_to_inputs(data, toker: PreTrainedTokenizer, **kwargs):\n",
        "    assert 'max_input_length' in kwargs, \"max_input_length should be given\"\n",
        "    process = lambda x: toker.convert_tokens_to_ids(\n",
        "        toker.tokenize(x, max_length=kwargs['max_input_length'], truncation=True)\n",
        "    )\n",
        "    token_num = toker.vocab_size\n",
        "    fine_to_coarse = {\n",
        "        0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 13: 0, 14: 0, 5: 1, 6: 1, 7: 1, 8: 1, 9: 2, 10: 2, 11: 2, 12: 2, -100: -100\n",
        "    }\n",
        "\n",
        "    context = _norm(' '.join(data[\"context\"]))\n",
        "    context = process(context)\n",
        "    dialog_id = data[\"dialog_id\"]\n",
        "    responses = data['responses']\n",
        "    inputs = []\n",
        "\n",
        "    for reply_anno in responses:\n",
        "        fine_ids = [process(f\"[{anno}]\")[0] - token_num if anno != \"-\" else -100 for anno in reply_anno[1:]]\n",
        "        inputs.append({\n",
        "            \"dialog_id\": dialog_id,\n",
        "            \"context\": context,\n",
        "            \"response\": process(_norm(reply_anno[0])),\n",
        "            \"label\": [fine_to_coarse[id] for id in fine_ids],\n",
        "        })\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def convert_inputs_to_features(inputs, toker: PreTrainedTokenizer, **kwargs):\n",
        "    if len(inputs) == 0:\n",
        "        return []\n",
        "\n",
        "    assert kwargs.get('max_input_length', None) != None, 'you should give max_input_length'\n",
        "    max_input_length = kwargs.get('max_input_length')\n",
        "\n",
        "    pad = toker.pad_token_id\n",
        "    if pad is None:\n",
        "        pad = toker.eos_token_id\n",
        "        assert pad != None, 'either pad_token_id or eos_token_id should be provided'\n",
        "    bos = toker.bos_token_id\n",
        "    if bos is None:\n",
        "        bos = toker.cls_token_id\n",
        "        assert bos != None, 'either bos_token_id or cls_token_id should be provided'\n",
        "    eos = toker.eos_token_id\n",
        "    if eos is None:\n",
        "        eos = toker.sep_token_id\n",
        "        assert eos != None, 'either eos_token_id or sep_token_id should be provided'\n",
        "    features = []\n",
        "    for i, ipt in enumerate(inputs):\n",
        "        feat = featurize(bos, eos, ipt, max_input_length)\n",
        "        features.append(feat)\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# for training\n",
        "class FeatureDataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.features[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate(features: List[InputFeatures], toker: PreTrainedTokenizer, infer=False):\n",
        "        pad = toker.pad_token_id\n",
        "        if pad is None:\n",
        "            pad = toker.eos_token_id\n",
        "            assert pad != None, 'either pad_token_id or eos_token_id should be provided'\n",
        "        bos = toker.bos_token_id\n",
        "        if bos is None:\n",
        "            bos = toker.cls_token_id\n",
        "            assert bos != None, 'either bos_token_id or cls_token_id should be provided'\n",
        "        eos = toker.eos_token_id\n",
        "        if eos is None:\n",
        "            eos = toker.sep_token_id\n",
        "            assert eos != None, 'either eos_token_id or sep_token_id should be provided'\n",
        "\n",
        "        input_ids = pad_sequence(\n",
        "            [torch.tensor(f.input_ids, dtype=torch.long) for f in features], batch_first=True, padding_value=pad)\n",
        "        attention_mask = input_ids == pad\n",
        "        token_type_ids = pad_sequence(\n",
        "            [torch.tensor(f.token_type_ids, dtype=torch.long) for f in features], batch_first=True, padding_value=pad)\n",
        "\n",
        "        labels = pad_sequence([torch.tensor(f.label, dtype=torch.long) for f in features], batch_first=True)\n",
        "\n",
        "        res = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"token_type_ids\": token_type_ids,\n",
        "            \"labels\": labels,\n",
        "        }\n",
        "\n",
        "        return res\n",
        "\n",
        "\n",
        "# for dev\n",
        "class DynamicBatchingLoader(object):\n",
        "    \"\"\" this loader takes raw text file, used for validate perplexity \"\"\"\n",
        "\n",
        "    def __init__(self, toker, batch_size, **kwargs):\n",
        "        assert \"fold_num\" in kwargs, \"fold_num should be given\"\n",
        "        fold_num = kwargs[\"fold_num\"]\n",
        "        dev_index = json.load(open(f\"{PARENT}dev_index.json\", 'r'))[fold_num]\n",
        "        with open(f\"{PARENT}esc_data.pkl\", 'rb') as f:\n",
        "            self.data = pickle.load(f)\n",
        "        self.trunc_chunk = []\n",
        "        self.lens = []\n",
        "        for feat in self.data:\n",
        "            if feat.dialog_id in dev_index:\n",
        "                self.trunc_chunk.append(feat)\n",
        "                self.lens.append(feat.input_length)\n",
        "\n",
        "        self.toker = toker\n",
        "        self.bs = batch_size\n",
        "        self.num_examples = len(self.trunc_chunk)\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "    def __iter__(self, epoch=1):\n",
        "        if epoch > 0:\n",
        "            for i_epoch in range(epoch):\n",
        "                yield from self._iter_epoch()\n",
        "        else:\n",
        "            while True:\n",
        "                yield from self._iter_epoch()\n",
        "\n",
        "    def __len__(self):\n",
        "        return ceil(self.num_examples / self.bs)\n",
        "\n",
        "    def _iter_epoch(self):\n",
        "        try:\n",
        "            features = []\n",
        "            for feature in self.trunc_chunk:\n",
        "                features.append(feature)\n",
        "                if len(features) >= self.bs:\n",
        "                    batch = self._batch_feature(features)\n",
        "                    yield batch\n",
        "                    features = []\n",
        "\n",
        "        except StopIteration:\n",
        "            pass\n",
        "\n",
        "    def _batch_feature(self, features):\n",
        "        return FeatureDataset.collate(features, self.toker)\n",
        "\n",
        "\n",
        "# for inference\n",
        "def convert_infer_to_features(inputs, toker: PreTrainedTokenizer, **kwargs):\n",
        "    if len(inputs) == 0:\n",
        "        return {}\n",
        "\n",
        "    assert kwargs.get('max_input_length', None) != None, 'you should give max_input_length'\n",
        "    max_input_length = kwargs.get('max_input_length')\n",
        "\n",
        "    pad = toker.pad_token_id\n",
        "    if pad is None:\n",
        "        pad = toker.eos_token_id\n",
        "        assert pad != None, 'either pad_token_id or eos_token_id should be provided'\n",
        "    bos = toker.bos_token_id\n",
        "    if bos is None:\n",
        "        bos = toker.cls_token_id\n",
        "        assert bos != None, 'either bos_token_id or cls_token_id should be provided'\n",
        "    eos = toker.eos_token_id\n",
        "    if eos is None:\n",
        "        eos = toker.sep_token_id\n",
        "        assert eos != None, 'either eos_token_id or sep_token_id should be provided'\n",
        "\n",
        "    input_ids = [ipt + [eos] for ipt in inputs[\"inp_seq\"]]\n",
        "    input_ids = [ipt[-max_input_length:] for ipt in input_ids]\n",
        "\n",
        "    decoder_input_ids = inputs[\"out_seq\"]\n",
        "    bos_tensor = torch.ones(decoder_input_ids.size(0), 1, dtype=decoder_input_ids.dtype) * bos\n",
        "    decoder_input_ids = torch.cat((bos_tensor, decoder_input_ids), -1)\n",
        "\n",
        "    features = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"decoder_input_ids\": decoder_input_ids,\n",
        "    }\n",
        "    return features\n",
        "\n",
        "\n",
        "# for inference\n",
        "def prepare_infer_batch(features, pad=0):\n",
        "    input_ids = pad_sequence([torch.tensor(f.input_ids, dtype=torch.long) for f in features], batch_first=True,\n",
        "                             padding_value=pad)\n",
        "    attention_mask = input_ids == pad\n",
        "    token_type_ids = pad_sequence([torch.tensor(f.token_type_ids, dtype=torch.long) for f in features],\n",
        "                                  batch_first=True, padding_value=pad)\n",
        "\n",
        "    res = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"token_type_ids\": token_type_ids,\n",
        "    }\n",
        "\n",
        "    res['batch_size'] = res['input_ids'].size(0)\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def get_infer_batch(corpus, toker, **kwargs):\n",
        "    assert \"max_input_length\" in kwargs, \"max_input_length should be given\"\n",
        "    process = lambda x: toker.convert_tokens_to_ids(\n",
        "        toker.tokenize(x, max_length=kwargs[\"max_input_length\"], truncation=True)\n",
        "    )\n",
        "\n",
        "    if \"no_bar_info\" in kwargs:\n",
        "        bar = enumerate(corpus)\n",
        "    else:\n",
        "        bar = tqdm.tqdm(enumerate(corpus), total=len(corpus), desc=\"preference score computing\")\n",
        "\n",
        "    for sample_id, line in bar:\n",
        "        data = json.loads(line)\n",
        "        context = _norm(' '.join(data[\"context\"]))\n",
        "        context = process(context)\n",
        "        responses = data['responses']\n",
        "        inputs = []\n",
        "        for reply in responses:\n",
        "            inputs.append({\n",
        "                \"dialog_id\": sample_id,\n",
        "                \"context\": context,\n",
        "                \"response\": process(_norm(reply)),\n",
        "                \"label\": None,\n",
        "            })\n",
        "        features = convert_inputs_to_features(inputs, toker, **kwargs)\n",
        "\n",
        "        yield prepare_infer_batch(features, toker.pad_token_id), sample_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T14:21:45.663437Z",
          "iopub.status.busy": "2025-03-20T14:21:45.66314Z",
          "iopub.status.idle": "2025-03-20T14:21:45.681145Z",
          "shell.execute_reply": "2025-03-20T14:21:45.6804Z",
          "shell.execute_reply.started": "2025-03-20T14:21:45.663404Z"
        },
        "id": "2TppAphoIbb6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title preference modelling\n",
        "def compute_scores(preference_model_args, args, **dataloader_kwargs):\n",
        "    mode = preference_model_args.mode\n",
        "    kwargs = {'mode': mode}\n",
        "    toker, model = build_pref_model(\n",
        "        checkpoint=os.path.join(args.preference_model_dir, \"best.bin\"),\n",
        "        local_rank=args.local_rank,\n",
        "        args=preference_model_args,\n",
        "        **kwargs,\n",
        "    )\n",
        "    deploy_model(model, args)\n",
        "    model.eval()\n",
        "\n",
        "    input_file = os.path.join(args.candidate_dir, \"candidates.txt\")\n",
        "    preference_mark = args.preference_model_dir.split('/')[-1]\n",
        "    save_path = os.path.join(args.candidate_dir, f\"preference_score_{preference_mark}.npy\")\n",
        "\n",
        "    inputter = inputters[\"esc\"]()\n",
        "    with open(input_file, 'r', encoding='UTF-8') as f:\n",
        "        corpus = f.readlines()\n",
        "    infer_dataloader = inputter.infer_dataloader(\n",
        "        toker=toker,\n",
        "        corpus=corpus,\n",
        "        batch_size=args.eval_batch_size,\n",
        "        **dataloader_kwargs,\n",
        "    )\n",
        "\n",
        "    predictions = []\n",
        "    for batch, sample_idx in infer_dataloader:\n",
        "        batch = {k: v.to(args.device) if isinstance(v, Tensor) else v for k, v in batch.items()}\n",
        "        scores = model.predict(**batch)[:, -1].cpu().numpy()\n",
        "        predictions.append(scores)\n",
        "\n",
        "    np.save(save_path, predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T14:21:45.682382Z",
          "iopub.status.busy": "2025-03-20T14:21:45.682049Z",
          "iopub.status.idle": "2025-03-20T14:21:45.699013Z",
          "shell.execute_reply": "2025-03-20T14:21:45.69836Z",
          "shell.execute_reply.started": "2025-03-20T14:21:45.682354Z"
        },
        "id": "_FDR2--eCRq5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title model builder for data processor and preference modeling, Don't run otherwise!!\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "\n",
        "# This one should already be defined above as expected\n",
        "# try:\n",
        "#     from preference_modeling.models import models\n",
        "# except ModuleNotFoundError:\n",
        "#     from models import models\n",
        "\n",
        "\n",
        "from transformers import (AutoTokenizer, AutoModel, AutoConfig)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def boolean_string(s):\n",
        "    if s.lower() not in {'false', 'true'}:\n",
        "        raise ValueError('Not a valid boolean string')\n",
        "    return s.lower() == 'true'\n",
        "\n",
        "\n",
        "def build_model(only_toker=False, checkpoint=None, args=None, **kwargs):\n",
        "    config_file = f\"{PARENT}preference_model.json\"\n",
        "    if not os.path.exists(config_file):\n",
        "        raise ValueError\n",
        "\n",
        "    with open(config_file, 'r', encoding='utf-8') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    if 'model_name' not in config or 'pretrained_model_path' not in config:\n",
        "        raise ValueError\n",
        "    toker = AutoTokenizer.from_pretrained(config['pretrained_model_path'])\n",
        "\n",
        "    if only_toker:\n",
        "        if 'expanded_vocab' in config:\n",
        "            toker.add_tokens(config['expanded_vocab'], special_tokens=True)\n",
        "        return toker\n",
        "\n",
        "    # assert \"mode\" in kwargs   #already taken care of\n",
        "    # mode = kwargs.pop(\"mode\")\n",
        "    # Model = models[mode]\n",
        "    model = Model.from_pretrained(config['pretrained_model_path'], args)\n",
        "    model.classifier = None\n",
        "    for name, param in model.named_parameters():\n",
        "        if name.startswith('bert.'):\n",
        "            param.requires_grad = False\n",
        "\n",
        "    if config.get('custom_config_path', None) != None:\n",
        "        model = model(AutoConfig.from_pretrained(config['custom_config_path']))\n",
        "\n",
        "    if 'gradient_checkpointing' in config:\n",
        "        setattr(model.config, 'gradient_checkpointing', config['gradient_checkpointing'])\n",
        "\n",
        "    if 'expanded_vocab' in config:\n",
        "        toker.add_tokens(config['expanded_vocab'], special_tokens=True)\n",
        "    model.tie_tokenizer(toker)\n",
        "\n",
        "    if checkpoint != None:\n",
        "        logger.info('loading finetuned model from %s' % checkpoint)\n",
        "        model.load_state_dict(torch.load(checkpoint, map_location=torch.device('cpu')), strict=False)\n",
        "\n",
        "    return toker, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-20T14:21:45.700212Z",
          "iopub.status.busy": "2025-03-20T14:21:45.699942Z",
          "iopub.status.idle": "2025-03-20T14:21:45.716343Z",
          "shell.execute_reply": "2025-03-20T14:21:45.71549Z",
          "shell.execute_reply.started": "2025-03-20T14:21:45.70018Z"
        },
        "id": "sB_RCOYaNoZB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title eval model\n",
        "import torch\n",
        "import logging\n",
        "from torch import Tensor\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn import metrics\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def cal_entropy(generated):\n",
        "    etp_score = [0.0, 0.0, 0.0, 0.0]\n",
        "    div_score = [0.0, 0.0, 0.0, 0.0]\n",
        "    counter = [defaultdict(int), defaultdict(int), defaultdict(int), defaultdict(int)]\n",
        "    for gg in generated:\n",
        "        g = gg.rstrip().split()\n",
        "        for n in range(4):\n",
        "            for idx in range(len(g) - n):\n",
        "                ngram = ' '.join(g[idx:idx + n + 1])\n",
        "                counter[n][ngram] += 1\n",
        "    for n in range(4):\n",
        "        total = sum(counter[n].values()) + 1e-10\n",
        "        for v in counter[n].values():\n",
        "            etp_score[n] += - (v + 0.0) / total * (np.log(v + 0.0) - np.log(total))\n",
        "        div_score[n] = (len(counter[n].values()) + 0.0) / total\n",
        "    return etp_score, div_score\n",
        "\n",
        "\n",
        "def eval_model(model, eval_dataloader, infer, args):\n",
        "    logger.info('\\ncompute eval model loss, using eval mode, '\n",
        "                'please change it back to train after calling this function')\n",
        "    model.eval()\n",
        "    tot_loss = 0\n",
        "    tot_sample = 0\n",
        "    pointwise_loss = []\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "            batch = {k: v.to(args.device) if isinstance(v, Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "            outputs = model(**batch)\n",
        "            loss_sample = outputs[\"loss\"]\n",
        "            if torch.isnan(loss_sample).sum().cpu().long().numpy() > 0:\n",
        "                print(loss_sample)\n",
        "                exit()\n",
        "            num_sample = batch[\"input_ids\"].shape[0]\n",
        "            tot_loss += loss_sample.sum().item()\n",
        "            tot_sample += num_sample\n",
        "\n",
        "            if infer:\n",
        "                pointwise_loss.extend(loss_sample.sum(dim=-1).cpu().tolist())\n",
        "    mean_loss = tot_loss / tot_sample\n",
        "    return mean_loss, pointwise_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "093721a65e3544b0b5d0d8a977171cd1",
            "0dbcfd93a4974c59a1935f279ab51f20",
            "f5df5d2b24b94ad1b32f9deb08e29048",
            "344db2aea8fc4243a0c72590f7cd8c76",
            "fd5626ac50614b8aa1216d95e2296962"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-03-20T14:21:45.717915Z",
          "iopub.status.busy": "2025-03-20T14:21:45.717591Z",
          "iopub.status.idle": "2025-03-20T14:33:00.590046Z",
          "shell.execute_reply": "2025-03-20T14:33:00.58937Z",
          "shell.execute_reply.started": "2025-03-20T14:21:45.717885Z"
        },
        "id": "kyK0ertSxqCE",
        "outputId": "a89f3c45-f887-430f-a2fa-e0ebdaa02e32",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "js loss training\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "093721a65e3544b0b5d0d8a977171cd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dbcfd93a4974c59a1935f279ab51f20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5df5d2b24b94ad1b32f9deb08e29048",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "344db2aea8fc4243a0c72590f7cd8c76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd5626ac50614b8aa1216d95e2296962",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Model were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['multi_prefix.posterior.control_trans_enc.0.bias', 'multi_prefix.posterior.control_trans_enc.0.weight', 'multi_prefix.posterior.control_trans_enc.2.bias', 'multi_prefix.posterior.control_trans_enc.2.weight', 'multi_prefix.posterior.mlp.0.bias', 'multi_prefix.posterior.mlp.0.weight', 'multi_prefix.posterior.mlp.2.bias', 'multi_prefix.posterior.mlp.2.weight', 'multi_prefix.posterior.wte_enc.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "training:   0%|          | 0/2922 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1113: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "training: 100%|█████████▉| 2916/2922 [11:07<00:01,  4.37it/s, loss: 0.04 mean loss: 0.08 epoch: 6]\n"
          ]
        }
      ],
      "source": [
        "#@title train d-pm model\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
        "from os.path import join\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from torch import Tensor\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--task_name\", type=str, default=\"esc\", choices=[\"esc\", \"mic\", \"off\"])\n",
        "parser.add_argument(\"--mode\", type=str, default=\"d-PM\", choices=[\"d-PM\", \"soft\", \"major\", \"single\"])\n",
        "parser.add_argument(\"--fold_num\", type=int, default=1)\n",
        "parser.add_argument(\"--class_num\", type=int, default=3)\n",
        "parser.add_argument(\"--off_fine_task\", type=str, default=\"offensive\", choices=[\"offensive\", \"hate\", \"aggressive\"])\n",
        "\n",
        "parser.add_argument(\"--preseqlen\", type=int, default=10)\n",
        "parser.add_argument(\"--d_prefix\", type=int, default=512)\n",
        "parser.add_argument(\"--prefix_dropout\", type=float, default=0.2)\n",
        "parser.add_argument(\"--max_grad_norm\", type=float, default=1.0)\n",
        "parser.add_argument(\"--epoch_num\", type=int, default=6)\n",
        "parser.add_argument(\"--train_batch_size\", type=int, default=32)\n",
        "parser.add_argument(\"--eval_batch_size\", type=int, default=64)\n",
        "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=4)\n",
        "parser.add_argument(\"--valid_step\", type=int, default=20)#800\n",
        "\n",
        "parser.add_argument(\"--seed\", type=int, default=42)\n",
        "parser.add_argument(\"--learning_rate\", type=float, default=1e-4)\n",
        "parser.add_argument(\"--warmup_steps\", type=int, default=400)\n",
        "\n",
        "parser.add_argument(\"--fp16\", action=\"store_true\")\n",
        "parser.add_argument(\"--loss_scale\", type=float, default=0.0)\n",
        "parser.add_argument(\"--pbar\", type=bool, default=True)\n",
        "parser.add_argument(\"--chinese\", action=\"store_true\")\n",
        "parser.add_argument(\"--local_rank\", type=int, default=-1)\n",
        "parser.add_argument(\"--load_checkpoint\", type=str, default=None)\n",
        "parser.add_argument(\"--loss_fn\", type=str, default=\"js\", choices=[\"qr\", \"rq\", \"js\"])\n",
        "\n",
        "print(\"js loss training\")\n",
        "args, _ = parser.parse_known_args()\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "args.device = device\n",
        "n_gpu = torch.cuda.device_count()\n",
        "CACHE_EMPTY_STEP = 10000\n",
        "\n",
        "names = {\n",
        "        \"mode\": args.mode,\n",
        "        \"inputter_name\": args.task_name,\n",
        "        \"fold_num\": args.fold_num,\n",
        "        \"fine_task\": args.off_fine_task,\n",
        "    }\n",
        "\n",
        "inputter = Inputter()\n",
        "\n",
        "toker = build_model(only_toker=True)\n",
        "\n",
        "train_dataloader = inputter.train_dataloader(\n",
        "            toker=toker,\n",
        "            feature_dataset=inputter.train_dataset,\n",
        "            batch_size=args.train_batch_size,\n",
        "            **names\n",
        "        )\n",
        "\n",
        "if args.epoch_num != None:\n",
        "        args.num_optim_steps = args.epoch_num * (len(train_dataloader) // args.train_batch_size\n",
        "                                                 + int(len(train_dataloader) % args.train_batch_size != 0))\n",
        "\n",
        "eval_dataloader_loss = inputter.valid_dataloader(\n",
        "        toker=toker,\n",
        "        batch_size=args.eval_batch_size,\n",
        "        **names\n",
        "    )\n",
        "\n",
        "\n",
        "#model and optimizer\n",
        "_, model = build_model(checkpoint=args.load_checkpoint, args=args, **names)\n",
        "model = model.to(device)\n",
        "# params = [p.data for p in model.parameters()]  #only for distributed env\n",
        "# all_reduce_and_rescale_tensors(params, float(torch.distributed.get_world_size()))\n",
        "\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "total_params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'ln', 'LayerNorm.weight']  # no decay for bias and LayerNorm (ln)\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if p.requires_grad and not any(nd in n for nd in no_decay)],\n",
        "      'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if p.requires_grad and any(nd in n for nd in no_decay)],\n",
        "      'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, )\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=args.num_optim_steps\n",
        ")\n",
        "\n",
        "if args.fp16:\n",
        "    logger.info('in fp16, using FusedAdam')\n",
        "    from apex import amp\n",
        "\n",
        "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime('%Y-%m%d-%H%M')[2:]\n",
        "output_dir = OUTPUT   #can add timestep if wanted more organized folds nd files\n",
        "\n",
        "\n",
        "train_logger = open(join(output_dir, 'train_log.csv'), 'a+', buffering=1)\n",
        "eval_logger = open(join(output_dir, 'eval_log.csv'), 'a+', buffering=1)\n",
        "print('epoch\\tglobal_step\\tstep\\ttmp_loss\\tmean_loss\\tepoch_time', file=train_logger)\n",
        "print('epoch\\tglobal_step\\tstep\\tfreq_loss', file=eval_logger)\n",
        "\n",
        "\n",
        "global_step = 0\n",
        "lowest_eval_loss = 1\n",
        "step = 0\n",
        "epoch = 0\n",
        "\n",
        "pbar = tqdm(total=args.num_optim_steps, desc=f\"training\", position=0)\n",
        "\n",
        "while True:\n",
        "      model.train()\n",
        "      tr_loss, mean_loss, nb_tr_examples, nb_tr_steps = 0.0, 0.0, 0, 0\n",
        "      train_start_time_epoch = time.time()\n",
        "      for batch in train_dataloader:\n",
        "          batch = {k: v.to(device) if isinstance(v, Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "          batch.update({'global_step': global_step})\n",
        "          batch.update({'epoch': epoch})\n",
        "          batch.update({'warmup_steps': args.warmup_steps})\n",
        "          outputs = model(**batch)\n",
        "\n",
        "          loss = outputs[\"loss\"]\n",
        "          predictions = outputs[\"dist\"].cpu()\n",
        "          if 'input_ids' in batch:\n",
        "              input_ids = batch['input_ids']\n",
        "          elif 'tgt_input_ids' in batch:\n",
        "              input_ids = batch['tgt_input_ids']\n",
        "          else:\n",
        "              assert 'src_input_ids' in batch\n",
        "              input_ids = batch['src_input_ids']\n",
        "\n",
        "          if n_gpu > 1:\n",
        "              loss = loss.mean()\n",
        "          loss = loss / (args.train_batch_size * args.gradient_accumulation_steps / input_ids.shape[0])\n",
        "\n",
        "          if args.fp16:\n",
        "              with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                  scaled_loss.backward()\n",
        "          else:\n",
        "              loss.backward()\n",
        "\n",
        "          tmp_loss = float(loss.item()) * (\n",
        "                  args.train_batch_size * args.gradient_accumulation_steps / input_ids.shape[0])\n",
        "          tr_loss += tmp_loss\n",
        "          nb_tr_examples += input_ids.size(0)\n",
        "          nb_tr_steps += 1\n",
        "          mean_loss = tr_loss / nb_tr_steps\n",
        "\n",
        "          # gradient update\n",
        "          step += 1\n",
        "          if step % args.gradient_accumulation_steps == 0:\n",
        "              if args.fp16:\n",
        "                  torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "              else:\n",
        "                  torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "              \"\"\"\n",
        "              if args.local_rank != -1:\n",
        "                  grads = [p.grad.data for p in model.parameters()\n",
        "                            if p.requires_grad and p.grad != None]\n",
        "                  all_reduce_and_rescale_tensors(grads, float(1))\n",
        "              \"\"\"\n",
        "\n",
        "              optimizer.step()\n",
        "              scheduler.step()\n",
        "              model.zero_grad()\n",
        "              global_step += 1\n",
        "\n",
        "\n",
        "              epoch_time = time.time() - train_start_time_epoch\n",
        "              pbar_str = ''  # f\"tok/s: {n_token_real_all_proc//epoch_time//1000}k \"\n",
        "              pbar_str += f\"loss: {tmp_loss:.2f} \"\n",
        "              pbar_str += f\"mean loss: {mean_loss:.2f} epoch: {epoch + 1}\"\n",
        "\n",
        "              pbar.set_postfix_str(pbar_str)\n",
        "              if args.epoch_num != None:\n",
        "                  pbar.update(args.gradient_accumulation_steps)\n",
        "              else:\n",
        "                  pbar.update(1)\n",
        "\n",
        "              print(f'{epoch + 1}\\t{global_step}\\t{step}\\t{tmp_loss}\\t{mean_loss}\\t{epoch_time}',\n",
        "                    file=train_logger)\n",
        "\n",
        "              if global_step % args.valid_step == 0:  # and epoch > 0:\n",
        "                  # only rank 0 process evaluate\n",
        "                  eval_loss, *_, = eval_model(\n",
        "                      model=model,\n",
        "                      eval_dataloader=eval_dataloader_loss,\n",
        "                      infer=False,\n",
        "                      args=args,\n",
        "                  )\n",
        "                  logger.info(f'**Eval (step {global_step})** eval_loss: {eval_loss}')\n",
        "                  print(f'{epoch + 1}\\t{global_step}\\t{step + 1}\\t{eval_loss}', file=eval_logger)\n",
        "                  logger.info('current learning rate: ' + str(optimizer.param_groups[0]['lr']))\n",
        "\n",
        "                  if eval_loss < lowest_eval_loss:\n",
        "                      torch.save(model.state_dict(), join(output_dir, f'model_step-{global_step}.bin'))\n",
        "                      torch.save(model.state_dict(), join(output_dir, 'best.bin'))\n",
        "                      toker.save_vocabulary(output_dir)\n",
        "                      try:\n",
        "                          model.config.to_json_file(join(output_dir, f'config.json'))\n",
        "                      except AttributeError:\n",
        "                          model.module.config.to_json_file(join(output_dir, f'config.json'))\n",
        "                      lowest_eval_loss = eval_loss\n",
        "                  model.train()\n",
        "\n",
        "              if args.epoch_num is None and global_step >= args.num_optim_steps:\n",
        "                  break\n",
        "\n",
        "          if (step + 1) % CACHE_EMPTY_STEP == 0:\n",
        "              torch.cuda.empty_cache()\n",
        "      epoch += 1\n",
        "\n",
        "      if epoch == args.epoch_num:\n",
        "        break\n",
        "pbar.close()\n",
        "train_logger.close()\n",
        "eval_logger.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrbWp6w-TKYL"
      },
      "source": [
        "################################################################################\n",
        "\n",
        "#******************************************************************************\n",
        "\n",
        "###############################################################################\n",
        "# Alignment training\n",
        "\n",
        "## alignment strategy:\n",
        "\n",
        "first using pretrained model we create candidates\n",
        "\n",
        "once candidates created, we score them using the preference model\n",
        "\n",
        "once scored, the dataset is created using the candidates and scores."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "preference_trainer",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 6846517,
          "sourceId": 10998318,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
